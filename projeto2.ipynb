{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 19/Set até às 23:59.<br />\n",
    "Grupo: 2 ou 3 pessoas - grupos com 3 pessoas terá uma rubrica diferenciada.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO gravar a key do professor no arquivo**\n",
    "\n",
    "\n",
    "### Entrega Intermediária: Check 1 - APS 2\n",
    "\n",
    "Até o dia 10/Set às 23:59, xlsx deve estar no Github com as seguintes evidências: \n",
    "\n",
    "  * Produto escolhido.\n",
    "  * Arquivo Excel contendo a base de treinamento e a base de testes já classificadas.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Parte I - Adquirindo a Base de Dados\n",
    "\n",
    "Acessar o notebook **Projeto-2-Planilha** para realizar a coleta dos dados. O grupo deve classificar os dados coletados manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from math import log\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "\n",
    "# Variables to be used\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Parte II - Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Não se esqueça de implementar o Laplace Smoothing (https://en.wikipedia.org/wiki/Laplace_smoothing).\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n",
    "Escreva o seu código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    \n",
    "    # Cleaning unwanted characters on the sentence\n",
    "    string = sentence.replace(\":\", \" \")\n",
    "    string = string.replace(\";\", \" \")\n",
    "    string = string.replace(\",\", \" \")\n",
    "    string = string.replace(\"?\", \" \")\n",
    "    string = string.replace(\"(\", \" \")\n",
    "    string = string.replace(\")\", \" \")\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    string = string.replace(\"'\", \" \")\n",
    "    string = string.replace('\"', \" \")\n",
    "    string = string.replace(\"!\", \" \")\n",
    "    string = string.replace(\"em smart fit\", \" \")\n",
    "    string = string.replace(\"smart fit\", \" \")\n",
    "    string = string.replace(\"2018\", \" \")\n",
    "    string = string.lower()\n",
    "\n",
    "    # Converting the sentence into a list of words\n",
    "    string_list = []\n",
    "    for word in string.split():\n",
    "        if word not in stop_words:\n",
    "            string_list.append(stemmer.stem(word))\n",
    "    \n",
    "    # Returning the clean list\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the DataSet\n",
    "df_train = pd.read_excel('smartfit_naivebayes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ec171f83003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Counting the total of each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcount_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcount_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcount_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifição\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Counting the total of each class\n",
    "count_0 = df_train.Classifição[df_train.Classifição == 0].count()\n",
    "count_1 = df_train.Classifição[df_train.Classifição == 1].count()\n",
    "count_2 = df_train.Classifição[df_train.Classifição == 2].count()\n",
    "\n",
    "total_count = df_train.Classifição.count()\n",
    "\n",
    "# Discovering the probability of each class\n",
    "P_0 = count_0/total_count\n",
    "P_1 = count_1/total_count \n",
    "P_2 = count_2/total_count\n",
    "\n",
    "print(\"Probabilidade de Negativo: {:.2f}%\".format(P_0*100))\n",
    "print(\"Probabilidade de Irrelevante: {:.2f}%\".format(P_1*100))\n",
    "print(\"Probabilidade de Positivo: {:.2f}%\".format(P_2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list to find out our d factor of the LaPlace Smoothing\n",
    "possible_words = []\n",
    "\n",
    "for sentence in df_train.Treinamento:\n",
    "    for word in clean_sentence(sentence):\n",
    "        if word not in possible_words:\n",
    "            possible_words.append(word)\n",
    "            \n",
    "d = len(possible_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dictionary with the words that appear in each Class of sentences, and how many times it appears\n",
    "# Also Creates a variable with the total of words for that Class of Sentences\n",
    "\n",
    "# Class 0\n",
    "words_0 = {}\n",
    "number_words_0 = 0\n",
    "\n",
    "for sentence in df_train.Treinamento[df_train.Classifição == 0]:\n",
    "    for word in clean_sentence(sentence):\n",
    "        if word in words_0:\n",
    "            words_0[word] += 1\n",
    "        else:\n",
    "            number_words_0 += 1\n",
    "            words_0[word] = 1\n",
    "            \n",
    "# Class 1\n",
    "words_1 = {}\n",
    "number_words_1 = 0\n",
    "\n",
    "for sentence in df_train.Treinamento[df_train.Classifição == 1]:\n",
    "    for word in clean_sentence(sentence):\n",
    "        if word in words_1:\n",
    "            words_1[word] += 1\n",
    "        else:\n",
    "            number_words_1 += 1\n",
    "            words_1[word] = 1\n",
    "            \n",
    "# Class 2\n",
    "words_2 = {}\n",
    "number_words_2 = 0\n",
    "\n",
    "for sentence in df_train.Treinamento[df_train.Classifição == 2]:\n",
    "    for word in clean_sentence(sentence):\n",
    "        if word in words_2:\n",
    "            words_2[word] += 1\n",
    "        else:\n",
    "            number_words_2 += 1\n",
    "            words_2[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(sentence, words, number_words, P, d, alpha=1):\n",
    "    prob = log(P)\n",
    "    \n",
    "    # Alphha factor for the LaPlace smoothing\n",
    "    total = number_words + alpha*d\n",
    "    for word in clean_sentence(sentence):\n",
    "        occurancy = alpha\n",
    "        if word in words:\n",
    "            occurancy += words[word]\n",
    "        prob += log(occurancy/total)\n",
    "            \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_probs(prob_0, prob_1, prob_2):\n",
    "\n",
    "    if prob_0 > prob_1 and prob_0 > prob_2:\n",
    "        return 0\n",
    "    elif prob_1 > prob_0 and prob_1 > prob_2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Classifier with the test set\n",
    "df_test = pd.read_excel(\"smartfit_naivebayes.xlsx\", sheet_name=1)\n",
    "\n",
    "alpha = 10\n",
    "\n",
    "classification = []\n",
    "for i in range(len(df_test.Teste)):\n",
    "    sentence = df_test.loc[i].Teste\n",
    "    prob_0 = calc_prob(sentence, words_0, number_words_0, P_0, d, alpha=alpha)\n",
    "    prob_1 = calc_prob(sentence, words_1, number_words_1, P_1, d, alpha=alpha)\n",
    "    prob_2 = calc_prob(sentence, words_2, number_words_2, P_2, d, alpha=alpha)\n",
    "    classif = comparing_probs(prob_0, prob_1, prob_2)\n",
    "    classification.append(classif)\n",
    "    \n",
    "df_test[\"Prediction\"] = classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de Acerto: 66.50%\n"
     ]
    }
   ],
   "source": [
    "cm = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "count = 0\n",
    "for i in range(len(df_test)):\n",
    "    if df_test.loc[i].Classifição == df_test.loc[i].Prediction:\n",
    "        count += 1\n",
    "        if df_test.loc[i].Classifição == 0:\n",
    "            cm[0][0] += 1\n",
    "        elif df_test.loc[i].Classifição == 1:\n",
    "            cm[1][1] += 1\n",
    "        else:\n",
    "            cm[2][2] += 1\n",
    "    if df_test.loc[i].Classifição == 0:\n",
    "        if df_test.loc[i].Prediction == 1:\n",
    "            cm[0][1] += 1\n",
    "        else:\n",
    "            cm[0][2] += 1\n",
    "    elif df_test.loc[i].Classifição == 1:\n",
    "        if df_test.loc[i].Prediction == 0:\n",
    "            cm[1][0] += 1\n",
    "        else:\n",
    "            cm[1][2] == 1\n",
    "    else:\n",
    "        if df_test.loc[i].Prediction == 0:\n",
    "            cm[2][0] += 1\n",
    "        else:\n",
    "            cm[2][1] += 1\n",
    "        \n",
    "\n",
    "print(\"Porcentagem de Acerto: {:.2f}%\".format(100*count/len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16,  6, 24],\n",
       "       [19, 94,  0],\n",
       "       [25, 26, 17]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite uma fraseeu odeio a smart fit, aquele lugar é muito ruim. um coco\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Digite uma frase \")\n",
    "\n",
    "prob_0 = calc_prob(sentence, words_0, number_words_0, 0.55, d, alpha=alpha)\n",
    "prob_1 = calc_prob(sentence, words_1, number_words_1, 0.215, d, alpha=alpha)\n",
    "prob_2 = calc_prob(sentence, words_2, number_words_2, 0.235, d, alpha=alpha)\n",
    "classif = comparing_probs(prob_0, prob_1, prob_2)\n",
    "print(classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_p0 = [0.55]\n",
    "acertividade = {}\n",
    "highest=[[\"\", 0]]\n",
    "\n",
    "for p_0 in possible_p0:\n",
    "    possible_p1 = np.arange(0.233, 0.238, 0.0001)\n",
    "    for p_1 in possible_p1:\n",
    "        p_2 = 1 - p_0 - p_1\n",
    "        combination = \"{}, {}, {}\".format(p_0, p_1, p_2)\n",
    "        counter = 0\n",
    "        for i in range(len(df_test.Teste)):\n",
    "            sentence = df_test.loc[i].Teste\n",
    "            prob_0 = calc_prob(sentence, words_0, number_words_0, p_0, d, alpha=alpha)\n",
    "            prob_1 = calc_prob(sentence, words_1, number_words_1, p_1, d, alpha=alpha)\n",
    "            prob_2 = calc_prob(sentence, words_2, number_words_2, p_2, d, alpha=alpha)\n",
    "            classif = comparing_probs(prob_0, prob_1, prob_2)\n",
    "            if classif == df_test.loc[i].Classifição:\n",
    "                counter += 1\n",
    "        acertividade[combination] = 100*counter/len(df_test)\n",
    "        if acertividade[combination] > highest[0][1]:\n",
    "            highest[0] = [combination, acertividade[combination]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_alpha = np.arange(9, 15, 0.25)\n",
    "highest_alpha = [[0, 0]]\n",
    "possible = {}\n",
    "\n",
    "for a in possible_alpha:\n",
    "    counter = 0\n",
    "    for i in range(len(df_test.Teste)):\n",
    "        sentence = df_test.loc[i].Teste\n",
    "        prob_0 = calc_prob(sentence, words_0, number_words_0, 0.55, d, alpha=a)\n",
    "        prob_1 = calc_prob(sentence, words_1, number_words_1, 0.215, d, alpha=a)\n",
    "        prob_2 = calc_prob(sentence, words_2, number_words_2, 0.235, d, alpha=a)\n",
    "        classif = comparing_probs(prob_0, prob_1, prob_2)\n",
    "        if classif == df_test.loc[i].Classifição:\n",
    "            counter += 1\n",
    "    possible[a] = 100*counter/len(df_test)\n",
    "    if possible[a] > highest_alpha[0][1]:\n",
    "        highest_alpha[0] = [a, possible[a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9.0: 70.5,\n",
       " 9.25: 70.5,\n",
       " 9.5: 70.5,\n",
       " 9.75: 70.5,\n",
       " 10.0: 70.5,\n",
       " 10.25: 70.5,\n",
       " 10.5: 70.5,\n",
       " 10.75: 70.5,\n",
       " 11.0: 70.5,\n",
       " 11.25: 70.5,\n",
       " 11.5: 70.5,\n",
       " 11.75: 70.5,\n",
       " 12.0: 70.5,\n",
       " 12.25: 70.5,\n",
       " 12.5: 70.5,\n",
       " 12.75: 70.5,\n",
       " 13.0: 70.5,\n",
       " 13.25: 70.5,\n",
       " 13.5: 70.0,\n",
       " 13.75: 70.0,\n",
       " 14.0: 70.0,\n",
       " 14.25: 70.0,\n",
       " 14.5: 70.0,\n",
       " 14.75: 70.0}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.55, 0.2135, 0.23649999999999996', 70.5]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.55, 0.21, 0.23999999999999996': 69.5,\n",
       " '0.55, 0.2105, 0.23949999999999996': 69.5,\n",
       " '0.55, 0.211, 0.23899999999999996': 69.5,\n",
       " '0.55, 0.2115, 0.23849999999999996': 69.5,\n",
       " '0.55, 0.212, 0.23799999999999996': 69.5,\n",
       " '0.55, 0.2125, 0.23749999999999996': 70.0,\n",
       " '0.55, 0.213, 0.23699999999999996': 70.0,\n",
       " '0.55, 0.2135, 0.23649999999999996': 70.5,\n",
       " '0.55, 0.214, 0.23599999999999996': 70.5,\n",
       " '0.55, 0.2145, 0.23549999999999996': 70.5,\n",
       " '0.55, 0.215, 0.23499999999999996': 70.5,\n",
       " '0.55, 0.2155, 0.23449999999999996': 70.5,\n",
       " '0.55, 0.216, 0.23399999999999996': 70.5,\n",
       " '0.55, 0.2165, 0.23349999999999996': 70.5,\n",
       " '0.55, 0.217, 0.23299999999999996': 70.5,\n",
       " '0.55, 0.2175, 0.23249999999999996': 70.5,\n",
       " '0.55, 0.218, 0.23199999999999996': 70.5,\n",
       " '0.55, 0.2185, 0.23149999999999996': 70.5,\n",
       " '0.55, 0.219, 0.23099999999999996': 70.5,\n",
       " '0.55, 0.2195, 0.23049999999999995': 70.5,\n",
       " '0.55, 0.22, 0.22999999999999995': 70.5,\n",
       " '0.55, 0.2205, 0.22949999999999995': 70.5,\n",
       " '0.55, 0.221, 0.22899999999999995': 70.5,\n",
       " '0.55, 0.2215, 0.22849999999999995': 70.5,\n",
       " '0.55, 0.222, 0.22799999999999995': 70.5,\n",
       " '0.55, 0.2225, 0.22749999999999995': 70.5,\n",
       " '0.55, 0.223, 0.22699999999999995': 70.5,\n",
       " '0.55, 0.2235, 0.22649999999999995': 70.5,\n",
       " '0.55, 0.224, 0.22599999999999995': 70.5,\n",
       " '0.55, 0.2245, 0.22549999999999995': 70.5,\n",
       " '0.55, 0.225, 0.22499999999999995': 70.0}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acertividade_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Obrigatório para grupos de 3 alunos:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
